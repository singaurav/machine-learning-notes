\documentclass[11pt, a4paper]{article}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{amsmath}
\usepackage{placeins}

\begin{document}

\title{LINEAR REGRESSION}
\date{}
\maketitle

Linear regression models assume a linear relationship between the inputs $X_1, X_2,\ ...,\ X_p$ and the output $Y$. These models are simple and often provide an insight into the effect of input variables on the output variable.
Linear models can be expanded to transformations of the input variables, thus making them widely applicable.

\section{Single Variable Regression}

Given training data of the form $(x_1, y_1), (x_2, y_2),\ ...,\ (x_N, y_N)$; the idea is to come up with the best estimate $\hat{y}$ such that, 

\begin{align*}
	\hat{y} = \beta_0 + \beta_1x 
\end{align*}

This scheme makes an estimation error at each point,
\begin{align*}
	\epsilon_i & = y_i - \hat{y}_i            \\
	           & = y_i - (\beta_0 + \beta_1x) 
\end{align*}

Generally, sum of squares of all the $N$ errors is minimized to define the best fit. Thus, the objective is to choose $\beta_0$ and $\beta_1$ such that this Residual Square Sum (RSS) is as small as possible. The errors are visualized below,

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
					    
		\draw[-latex] (0,0) -- (6,0) node[right]{x};
		\draw[-latex] (0,0) -- (0,5) node[left]{y};	
				
		\draw (0, 1) -- (5, 4);
				
		\draw (5, 4) node [above right] {$y = \beta_0 + \beta_1x$};
		\draw (0, 1) node [left] {$\beta_0$};
				
		\foreach \Point in {(1,3), (2, 3), (4, 4), (3, 2), (1.5, 1)}{
			\node at \Point {\textbullet};
		};
		        
		\draw (1, 3) node [above] {$x_1, y_1$};
		\draw (1.5, 1) node [below] {$x_2, y_2$};
		\draw (2, 3) node [above right] {$x_3, y_3$};
		\draw (3, 2) node [below] {$x_4, y_4$};
		\draw (4, 4) node [above] {$x_5, y_5$};
		        
		\draw [thick] (1, 3) -- (1, 1.6);
		\draw [thick] (1.5, 1) -- (1.5, 1.9);
		\draw [thick] (2, 3) -- (2, 2.2);
		\draw [thick] (3, 2) -- (3, 2.8);
		\draw [thick] (4, 4) -- (4, 3.4);        						
	\end{tikzpicture}
\end{figure}

Note that as line moves away from the data points the $RSS$ gets bigger since each error becomes bigger. In fact there is no upper bound on $RSS$, only a lower bound. Hence to get the minimum $RSS$, equating partial differentials w.r.t. $\beta_0$ and $\beta_1$ to zero is sufficient without worrying if the extrema is maxima or minima, it is always guaranteed to be a minima.

\begin{align*}
	RSS(\beta_0, \beta_1)                 & = \sum\limits_{i = 1}^N (y_i - \beta_0 - \beta_1x_i)^2    \\
	\frac{\partial RSS}{\partial \beta_0} & = -2\sum\limits_{i = 1}^N (y_i - \beta_0 - \beta_1x_i)    \\
	\frac{\partial RSS}{\partial \beta_1} & = -2\sum\limits_{i = 1}^N (y_i - \beta_0 - \beta_1x_i)x_i \\
\end{align*}

Equating both to zero,

\begin{align*}
	\beta_0N + \beta_1\sum x_i = \sum y_i             \\
	\beta_0\sum x_i + \beta_1\sum x_i^2 = \sum x_iy_i 
\end{align*}

This can be written in matrix form as follows,

\begin{align*}
	\begin{pmatrix} N & \sum x_i \\ \sum x_i & \sum x_i^2 \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix} = \begin{pmatrix}
	\sum y_i \\ \sum x_iy_i \end{pmatrix}
\end{align*}


Multiplying by inverse of the $2\times2$ matrix on both sides,

\begin{align*}
	\begin{pmatrix} \beta_0 &                                                                   &           \\ \beta_1 \end{pmatrix} &= \begin{pmatrix} N & \sum x_i \\ \sum x_i & \sum x_i^2 \end{pmatrix}^{-1} \begin{pmatrix}
	\sum y_i \\ \sum x_iy_i \end{pmatrix} \\
	                        & = \frac{1}{N\sum x_i^2 - (\sum x_i)^2} \begin{pmatrix} \sum x_i^2 & -\sum x_i \\ -\sum x_i & N \end{pmatrix} \begin{pmatrix}
	\sum y_i \\ \sum x_iy_i \end{pmatrix} \\
	&= \frac{1}{N\sum x_i^2 - (\sum x_i)^2} \begin{pmatrix}
	\sum x_i^2 \sum y_i -\sum x_iy_i  \sum x_i  \\ N \sum x_i y_i - \sum x_i \sum y_i \end{pmatrix} \\
\end{align*}

Dividing both numerator and denominator by $N^2$, $\beta_0$ and $\beta_1$ can be represented in terms of averages as follows, 

\begin{align*}
	\beta_0 & = \frac{Avg(x^2)Avg(y) - Avg(xy)Avg(x)}{Avg(x^2) - [Avg(x)]^2}                    \\
	\beta_1 & = \frac{Avg(xy) - Avg(x)Avg(y)}{Avg(x^2) - [Avg(x)]^2} = \frac{Cov(x, y)}{Var(x)} \\
\end{align*}

After the calculation of $\beta_0$ and $\beta_1$, the goodness of fit is measured by correlation,

\begin{align*}
	r = \frac{Cov(x, y)}{\sqrt{Var(x)Var(y)}} 
\end{align*}

Correlation always resides in the interval $[-1, 1]$. It is 1 or -1 if all the data points $(x_i, y_i)$ lie on a line.

\subsection{Input Transformations}

\begin{itemize}
	\item Relationship between $x$ and $y$ is not linear if $y = x^n$. Taking $log$ on both sides, $log(y) = nlog(x)$. Clearly relationship between $log(x)$ and $log(y)$ is linear. 
	\item Relationship between $x$ and $y$ is not linear if $y = a^x$. Taking $log$ on both sides, $log(y) = xlog(a)$. Clearly, relationship between $log(y)$ and $x$ is linear.
	\item Due to scenarios like this, the inputs are sometimes transformed appropriately before being fed into a linear regression model.
\end{itemize}
 
\section{Multiple Variable Regression} 

\end{document}