\documentclass[11pt, a4paper]{article}

\usepackage{amsmath}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{kbordermatrix}
\usetikzlibrary{arrows,automata, shapes, petri}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{booktabs}

\begin{document}

\title{HIDDEN MARKOV MODELS}
\date{}
\maketitle

Hidden Markov Models(HMMs) are statistical tools to model sequential observations with the assumption that states of the system generating them follow a Markov process but these states are unobservable/hidden. However, an observation at any point is related to the underlying hidden state the system is in at that point.

\section{Example}

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=5cm,
		semithick]
						
		\node[initial,state] (I)                    {$$};
		\node[state]         (S) [below left of=I] {$S$};
		\node[state]         (T) [below right of=I] {$T$};
		\node[rectangle]         (SX) [below left of=S,yshift=-1.2cm,xshift=1cm]     {$X$};
		\node[rectangle]         (SY) [below of=S]     {$Y$};  
		\node[rectangle]         (SZ) [below right of=S,yshift=-1.2cm,xshift=-1cm]     {$Z$};
		\node[rectangle]         (TX) [below left of=T,yshift=-1.2cm,xshift=1cm]     {$X$};
		\node[rectangle]         (TY) [below of=T]     {$Y$};  
		\node[rectangle]         (TZ) [below right of=T,yshift=-1.2cm,xshift=-1cm]     {$Z$};  
						
		\path (I) edge [bend right]    node {0.6} (S)
		edge [bend left]     node {0.4} (T)
		(S) edge [loop left]    node {0.7} (S)
		edge [bend left]    node {0.3} (T)
		edge                node {0.1} (SX)
		edge                node {0.4} (SY)
		edge                node {0.5} (SZ)
		(T) edge [loop right]   node {0.6} (T)
		edge [bend left]    node {0.4} (S)
		edge                node {0.7} (TX)
		edge                node {0.2} (TY)
		edge                node {0.1} (TZ);   
		\draw [to-to] (-7,-7) -- (7,-7); 
		\node at (0, -6.5) {Hidden};
		\node at (0, -7.5) {Observable};                   
	\end{tikzpicture}
\end{center}

In this diagram, nodes $\{S, T\}$ represent the hidden states and $\{X, Y, Z\}$ represent observable states. The unlabelled node is the start node. These states together with the associated probabilities fully characterize the HMM. The state transition probabilites denoted by $A$ can be represented as,

\[
	A = \kbordermatrix{
		& S & T \\
		S & 0.7 & 0.3 \\
		T & 0.6 & 0.4 \\
	}
\]

The emmission probabilities are denoted by $B$.

\[
	B = \kbordermatrix{
		& X & Y & Z \\
		S & 0.1 & 0.4 & 0.5 \\
		T & 0.7 & 0.2 & 0.1 \\
	}
\]

Lastly, initial state distribution is denoted by $\pi$.

\[
	\pi = \kbordermatrix{
		& S & T \\
		& 0.6 & 0.4  
	}
\]

\section{The three problems of HMM}

\subsection{The Evaluation Problem}

Given a HMM, $\lambda=(A, B, \pi)$ and a set of observations $O$, find $P(O|\lambda)$ (probability that the observations were generated by the model).

Consider the HMM in the example above and let $O = (ZXY)$.

\begin{align*}
	P(ZXY|\lambda) & = P(SSS, ZXY|\lambda)  \\
	               & +\ P(SST, ZXY|\lambda) \\
	               & +\ P(STS, ZXY|\lambda) \\
	               & +\ P(STT, ZXY|\lambda) \\
	               & +\ P(TSS, ZXY|\lambda) \\
	               & +\ P(TST, ZXY|\lambda) \\
	               & +\ P(TTS, ZXY|\lambda) \\
	               & +\ P(TTT, ZXY|\lambda) \\  
\end{align*}

Each of the terms on R.H.S. can be calculated using the following law of probability.

\begin{align*}
	P(a, b|c) & = \frac{P(a, b, c) }{P(c)}                            \\ 
	          & = \frac{P(a, c) }{P(c)} * \frac{P(a, b, c) }{P(a, c)} \\
	          & = P(a|c) * P(b|a,c)                                   
\end{align*} 

\begin{table}[h!]
	\centering
	\caption{$P(ZXY|\lambda)$ calculation}
	\label{tab:table1}
	\begin{tabular}{cccc|c}
		\toprule
		HS  & OS  & $P(HS|\lambda)$   & $P(OS|HS, \lambda)$ & $P(HS, OS|\lambda)$ \\
		\midrule
		SSS & ZXY & 0.6*0.7*0.7=0.294 & 0.5*0.1*0.4=0.020   & 0.005880            \\
		SST & ZXY & 0.6*0.7*0.3=0.126 & 0.5*0.1*0.2=0.010   & 0.001260            \\
		STS & ZXY & 0.6*0.3*0.4=0.072 & 0.5*0.7*0.4=0.140   & 0.010080            \\
		STT & ZXY & 0.6*0.3*0.6=0.108 & 0.5*0.7*0.2=0.070   & 0.007560            \\
		TSS & ZXY & 0.4*0.4*0.7=0.112 & 0.1*0.1*0.4=0.004   & 0.000448            \\
		TST & ZXY & 0.4*0.4*0.3=0.048 & 0.1*0.1*0.2=0.002   & 0.000096            \\
		TTS & ZXY & 0.4*0.6*0.4=0.096 & 0.1*0.7*0.4=0.028   & 0.002688            \\
		TTT & ZXY & 0.4*0.6*0.6=0.144 & 0.1*0.7*0.2=0.014   & 0.002016            \\
		\midrule
		    &     &                   & $P(ZXY|\lambda)$    & 0.030028            \\
		\bottomrule
	\end{tabular}
\end{table}

Calculation in this manner is expensive because probabilities corresponding to all permutations of hidden states of the same length as the length of observed sequence have to be calculated. 

To visualize the above calculation, consider the following diagrammatic represntation.

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		semithick]
						
		\node[state] (Start)                    {$Start$};
		\node[state] (S1) [above right of=Start] {$S$};
		\node[state] (T1) [below right of=Start] {$T$};
		\node[state] (S2) [right of=S1] {$S$};
		\node[state] (T2) [right of=T1] {$T$};
		\node[state] (S3) [right of=S2] {$S$};
		\node[state] (T3) [right of=T2] {$T$};
		\node[state] (End) [below right of=S3] {$End$};
		\node [rectangle, node distance=2cm] (Z) [above of=S1] {Z};
		\node [rectangle, node distance=2cm] (X) [above of=S2] {X};
		\node [rectangle, node distance=2cm] (Y) [above of=S3] {Y};						
						                
		\path (Start) edge  node {0.6} (S1)
		edge  node {0.4} (T1)
		(S1) edge  node {0.5*0.7} (S2)
		edge  node [pos=0.2] {0.5*0.3} (T2)
		(T1) edge  node [pos=0.2] {0.1*0.4} (S2)
		edge  node {0.1*0.6} (T2)
		(S2) edge  node {0.1*0.7} (S3)
		edge node [pos=0.2] {0.1*0.3} (T3)
		(T2) edge node [pos=0.2] {0.7*0.4} (S3)
		edge node {0.7*0.6} (T3)
		(S3) edge node {0.4} (End)
		(T3) edge node {0.2} (End);                     
	\end{tikzpicture}
\end{center}

The arrow from $S$ under $Z$ and $S$ under $X$ is labelled 0.5*0.7 as probability of emitting $Z$ given $S$ is 0.5 and probability of transition from $S$ to $S$ is 0.7. The arrow from $Start$ to $S$ is labelled 0.6 as initial probability of transitioning to $S$ is 0.6. Similarly, the arrow from $S$ to End is labelled 0.4 as probability of emitting $Y$ given $S$ is 0.4.

Every path in the diagram from $Start$ to $End$ has a special meaning in the sense that the multiplication of all labels on the path (which henceforth shall be called its value) is equal to the joint probability of the observation sequence and the hidden state transition sequence defined by the path given the model.

The diagram also leads to the fact that $P(ZXY|\lambda)$ can be thought of as arising out of the enumeration of all paths from $Start$ to $End$ and then summation over their value. 

Dynamic Programming can be used to circumvent enumeration and speed up the computation as shown below,


\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		semithick]
						
		\node[state] (Start)                    {$1$};
		\node[state] (S1) [above right of=Start] {$0.6$};
		\node[rectangle, node distance=2cm] (D1) [right of=S1] {$=1*0.6$};
		\node[state] (T1) [below right of=Start] {$0.4$};
		\node[rectangle, node distance=2cm] (D2) [right of=T1] {$=1*0.4$};		 
		\path (Start) edge  node {0.6} (S1)
		edge  node {0.4} (T1);                     
	\end{tikzpicture}
\end{center}

\noindent\rule{\textwidth}{0.6pt}

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		semithick]
						
		\node[state] (S1) {$0.6$};
		\node[state] (T1) [below of=S1] {$0.4$};
		\node[state] (S2) [right of=S1] {$0.226$};
		\node[rectangle, node distance=4cm] (D2) [right of=S2] {$=0.6*0.5*0.7 + 0.4*0.1*0.4$};	
		\node[state] (T2) [right of=T1] {$0.114$};
		\node[rectangle, node distance=4cm] (D2) [right of=T2] {$=0.6*0.5*0.3 + 0.4*0.1*0.6$};				
			                
		\path 
		(S1) edge  node {0.5*0.7} (S2)
		edge  node [pos=0.2] {0.5*0.3} (T2)
		(T1) edge  node [pos=0.2] {0.1*0.4} (S2)
		edge  node {0.1*0.6} (T2);		
	\end{tikzpicture}
\end{center}

\noindent\rule{\textwidth}{0.6pt}

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		semithick]
						
		\node[state] (S2) {$0.226$};
		\node[state] (T2) [below of=S2] {$0.114$};
		\node[state] (S3) [right of=S2] {$0.04774$};
		\node[rectangle, node distance=4cm] (D3) [right of=S3] {$=0.226*0.1*0.7 + 0.114*0.7*0.4$};
		\node[state] (T3) [right of=T2] {$0.05466$};
		\node[rectangle, node distance=4cm] (D3) [right of=T3] {$=0.226*0.1*0.3 + 0.114*0.7*0.6$};
					
						                
		\path 
		(S2) edge  node {0.1*0.7} (S3)
		edge node [pos=0.2] {0.1*0.3} (T3)
		(T2) edge node [pos=0.2] {0.7*0.4} (S3)
		edge node {0.7*0.6} (T3);                     
	\end{tikzpicture}
\end{center}

\noindent\rule{\textwidth}{0.6pt}

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		semithick]
						
		\node[state] (S3) {$0.04774$};
		\node[state] (T3) [below of=S3] {$0.05466$};
		\node[state] (End) [above right of=T3] {$0.030028$};
		\node[rectangle, node distance=4cm] (D4) [right of=End] {$=0.04774*0.4 + 0.05466*0.2$};				                
		\path 
		(S3) edge node {0.4} (End)
		(T3) edge node {0.2} (End);                     
	\end{tikzpicture}
\end{center}

In this algorithm, each node on the right keeps track of sum of values of all paths from $Start$ to that node denoted by $\alpha_t(state)$.

\begin{align*}
	\alpha_0(Start) & = 1.0      \\
	\alpha_1(S)     & = 0.6      \\
	\alpha_1(T)     & = 0.4      \\
	\alpha_2(S)     & = 0.226    \\
	\alpha_2(T)     & = 0.114    \\
	\alpha_3(S)     & = 0.04774  \\
	\alpha_3(T)     & = 0.05466  \\
	\alpha_4(End)   & = 0.030028 
\end{align*}

The same algorithm can be run backwards from the $End$ node and work its way towards the $Start$ node. Now, each node in the left will keep track of sum of values of all paths from $End$ to that node denoted by $\beta_t(state)$.

\begin{align*}
	\beta_4(Start) & = 1.0      \\
	\beta_3(S)     & = 0.4      \\
	\beta_3(T)     & = 0.2      \\
	\beta_2(S)     & = 0.034    \\
	\beta_2(T)     & = 0.196    \\
	\beta_1(S)     & = 0.0413   \\
	\beta_1(T)     & = 0.01312  \\
	\beta_0(End)   & = 0.030028 
\end{align*}

It is apparent that the following relation must hold,

\begin{align*}
	\sum_{\text{all states}} \alpha_t(state)*\beta_t(state) = 0.030028\  \forall\ t 
\end{align*}
 
\subsection{The Decoding Problem}

\subsection{The Learning Problem}


\end{document}